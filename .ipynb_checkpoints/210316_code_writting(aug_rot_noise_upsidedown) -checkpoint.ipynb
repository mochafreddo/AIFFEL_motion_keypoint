{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "union-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from math import cos, sin, pi\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incoming-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 이동\n",
    "os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elementary-terrorist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv',\n",
       " 'train_imgs.zip',\n",
       " 'baseline_submission.csv',\n",
       " 'train_df.csv',\n",
       " 'train_imgs',\n",
       " 'test_imgs',\n",
       " 'test_imgs.zip',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wicked-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터 중 10%를 검증 데이터로 사용\n",
    "\n",
    "# csv 파일 불러오기\n",
    "data = pd.read_csv('train_df.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 경로 설정\n",
    "data_paths = sorted(glob.glob('./train_imgs/*.jpg'))\n",
    "test_paths = sorted(glob.glob('./test_imgs/*.jpg'))\n",
    "\n",
    "data['path'] = data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 길이는:  3776\n",
      "검증 데이터 길이는:  419\n"
     ]
    }
   ],
   "source": [
    "# 데이터 프레임 랜덤하게 분할\n",
    "\n",
    "# 전체 데이터 중 90%는 학습 데이터 활용\n",
    "train = data.sample(frac=0.9, random_state=2021)\n",
    "print('학습 데이터 길이는: ', len(train))\n",
    "\n",
    "# 전체 데이터 중 10%는 검증 데이터 활용\n",
    "valid = data.drop(train.index)\n",
    "print('검증 데이터 길이는: ', len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "based-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좌우 반전\n",
    "def left_right_flip(images, keypoints):\n",
    "    flipped_keypoints = []\n",
    "    flipped_images = np.flip(images, axis=1)\n",
    "    for idx, sample_keypoints in enumerate(keypoints):\n",
    "        if idx%2 == 0:\n",
    "            flipped_keypoints.append(480.-sample_keypoints)\n",
    "        else:\n",
    "            flipped_keypoints.append(sample_keypoints)\n",
    "    \n",
    "    # left_right_keypoints_convert\n",
    "    for i in range(8):\n",
    "        flipped_keypoints[2+(4*i):4+(4*i)], flipped_keypoints[4+(4*i):6+(4*i)] = flipped_keypoints[4+(4*i):6+(4*i)], flipped_keypoints[2+(4*i):4+(4*i)]\n",
    "    flipped_keypoints[36:38], flipped_keypoints[38:40] = flipped_keypoints[38:40], flipped_keypoints[36:38]\n",
    "    flipped_keypoints[44:46], flipped_keypoints[46:48] = flipped_keypoints[46:48], flipped_keypoints[44:46]\n",
    "    \n",
    "    return flipped_images, flipped_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "domestic-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation Setting\n",
    "pixel_shifts = [12]\n",
    "rotation_angles = [12]\n",
    "inc_brightness_ratio = 1.2\n",
    "dec_brightness_ratio = 0.8\n",
    "noise_ratio = 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "taken-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수직/수평 동시 이동\n",
    "# forloop에서 shift_x, shift_y 중 하나만 놓으면\n",
    "# 수직 또는 수평 이동만 따로 시행 가능\n",
    "def shift_images(images, keypoints):\n",
    "    # tensor -> numpy\n",
    "    images = images.numpy()\n",
    "    shifted_images = []\n",
    "    shifted_keypoints = []\n",
    "    for shift in pixel_shifts:   \n",
    "        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n",
    "            # 이동할 matrix 생성\n",
    "            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\n",
    "            shifted_keypoint = np.array([])\n",
    "            shifted_x_list = np.array([])\n",
    "            shifted_y_list = np.array([])\n",
    "            # 이미지 이동\n",
    "            shifted_image = cv2.warpAffine(images, M, (480,270), flags=cv2.INTER_CUBIC)\n",
    "            # 이동한만큼 keypoint 수정\n",
    "            for idx, point in enumerate(keypoints):\n",
    "                if idx%2 == 0: \n",
    "                    shifted_keypoint = np.append(shifted_keypoint, point+shift_x)\n",
    "                    shifted_x_list = np.append(shifted_x_list, point+shift_x)\n",
    "                else: \n",
    "                    shifted_keypoint =np.append(shifted_keypoint, point+shift_y)\n",
    "                    shifted_y_list = np.append(shifted_y_list, point+shift_y)\n",
    "            # 수정된 keypoint가 이미지 사이즈를 벗어나지 않으면 append\n",
    "            if np.all(0.0<shifted_x_list) and np.all(shifted_x_list<480) and np.all(0.0<shifted_y_list) and np.all(shifted_y_list<270):\n",
    "                shifted_images.append(shifted_image.reshape(270,480,3))\n",
    "                shifted_keypoints.append(shifted_keypoint)\n",
    "\n",
    "    return shifted_images, shifted_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blind-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 회전\n",
    "def rotate_augmentation(images, keypoints):\n",
    "    # tensor -> numpy\n",
    "    images = images.numpy()\n",
    "    rotated_images = []\n",
    "    rotated_keypoints = []\n",
    "    \n",
    "    for angle in rotation_angles:\n",
    "        for angle in [angle,-angle]:\n",
    "            # 회전할 matrix 생성\n",
    "            M = cv2.getRotationMatrix2D((240,135), angle, 1.0)\n",
    "            # cv2_imshow로는 문제없지만 추후 plt.imshow로 사진을 확인할 경우 black screen 생성...\n",
    "            # 혹시 몰라 matrix를 ndarray로 변환\n",
    "            M = np.array(M, dtype=np.float32)\n",
    "            angle_rad = -angle*pi/180\n",
    "            rotated_image = cv2.warpAffine(images, M, (480,270))\n",
    "            rotated_images.append(rotated_image)\n",
    "            \n",
    "            # keypoint를 copy하여 forloop상에서 값이 계속 없데이트 되는 것을 회피\n",
    "            rotated_keypoint = keypoints.copy()\n",
    "            rotated_keypoint[0::2] = rotated_keypoint[0::2] - 240\n",
    "            rotated_keypoint[1::2] = rotated_keypoint[1::2] - 135\n",
    "            \n",
    "            for idx in range(0,len(rotated_keypoint),2):\n",
    "                rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\n",
    "                rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\n",
    "\n",
    "            rotated_keypoint[0::2] = rotated_keypoint[0::2] + 240\n",
    "            rotated_keypoint[1::2] = rotated_keypoint[1::2] + 135\n",
    "            rotated_keypoints.append(rotated_keypoint)\n",
    "        \n",
    "    return rotated_images, rotated_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atmospheric-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 해상도 조절\n",
    "def alter_brightness(images):\n",
    "    altered_brightness_images = []\n",
    "    inc_brightness_images = np.clip(images*inc_brightness_ratio, 0.0, 1.0)\n",
    "    dec_brightness_images = np.clip(images*dec_brightness_ratio, 0.0, 1.0)\n",
    "    altered_brightness_images.append(inc_brightness_images)\n",
    "    altered_brightness_images.append(dec_brightness_images)\n",
    "    return altered_brightness_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "little-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random 노이즈 추가\n",
    "def add_noise(images):\n",
    "    images = images.numpy()\n",
    "    noise = noise_ratio * np.random.randn(270,480,3)\n",
    "    noise = noise.astype(np.float32)\n",
    "    # 생성한 noise를 원본에 add\n",
    "    noisy_image = cv2.add(images, noise)\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "angry-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator():\n",
    "    # 원본 이미지 resize\n",
    "    for i in range(len(train)):\n",
    "        img = tf.io.read_file(train['path'][i]) # path(경로)를 통해 이미지 읽기\n",
    "        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\n",
    "        img = tf.image.resize(img, [270,480]) # 이미지 resize \n",
    "        img = img/255                         # 이미지 rescaling\n",
    "        target = train.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\n",
    "        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\n",
    "\n",
    "        yield (img, target)\n",
    "    \n",
    "    # horizontal flip\n",
    "#     for i in range(len(train)):\n",
    "#         img = tf.io.read_file(train['path'][i]) \n",
    "#         img = tf.image.decode_jpeg(img, channels=3) \n",
    "#         img = tf.image.resize(img, [270,480]) \n",
    "#         img = img/255\n",
    "#         target = train.iloc[:,1:49].iloc[i,:] \n",
    "#         target = target/4\n",
    "#         img, target = left_right_flip(img, target)\n",
    "        \n",
    "#         yield (img, target)\n",
    "\n",
    "    # Horizontal & Vertical shift\n",
    "    for i in range(len(train)):\n",
    "        img = tf.io.read_file(train['path'][i])\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [270,480])\n",
    "        img = img/255\n",
    "        target = train.iloc[:,1:49].iloc[i,:]\n",
    "        target = target/4\n",
    "        img_list, target_list = shift_images(img, target)\n",
    "        for shifted_img, shifted_target in zip(img_list, target_list):\n",
    "            \n",
    "            yield (shifted_img, shifted_target)\n",
    "\n",
    "    # Rotation\n",
    "    for i in range(len(train)):\n",
    "        img = tf.io.read_file(train['path'][i])\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [270,480])\n",
    "        img = img/255\n",
    "        target = train.iloc[:,1:49].iloc[i,:]\n",
    "        target = target/4\n",
    "        img_list, target_list = rotate_augmentation(img, target)\n",
    "        for rotated_img, rotated_target in zip(img_list, target_list):\n",
    "            \n",
    "            yield (rotated_img, rotated_target)\n",
    "\n",
    "    # Alter_Brightness\n",
    "#     for i in range(len(train)):\n",
    "#         img = tf.io.read_file(train['path'][i])\n",
    "#         img = tf.image.decode_jpeg(img, channels=3)\n",
    "#         img = tf.image.resize(img, [270,480])\n",
    "#         img = img/255\n",
    "#         target = train.iloc[:,1:49].iloc[i,:]\n",
    "#         target = target/4\n",
    "#         img_list = alter_brightness(img)\n",
    "#         for altered_brightness_images in img_list:\n",
    "            \n",
    "#             yield (altered_brightness_images, target)\n",
    "\n",
    "    # Adding_Noise\n",
    "    for i in range(len(train)):\n",
    "        img = tf.io.read_file(train['path'][i])\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [270,480])\n",
    "        img = img/255\n",
    "        target = train.iloc[:,1:49].iloc[i,:]\n",
    "        target = target/4\n",
    "        noisy_img = add_noise(img)\n",
    "\n",
    "        yield (noisy_img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "underlying-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validGenerator():\n",
    "    # 원본 이미지 resize\n",
    "    for i in range(len(valid)):\n",
    "        img = tf.io.read_file(valid['path'][i]) # path(경로)를 통해 이미지 읽기\n",
    "        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\n",
    "        img = tf.image.resize(img, [270,480]) # 이미지 resize \n",
    "        img = img/255                         # 이미지 rescaling\n",
    "        target = valid.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\n",
    "        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\n",
    "\n",
    "        yield (img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "offensive-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(1)\n",
    "valid_dataset = tf.data.Dataset.from_generator(validGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chief-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback 설정\n",
    "earlystop = EarlyStopping(patience=7)\n",
    "learning_rate_reduction=ReduceLROnPlateau(\n",
    "                        monitor= \"val_loss\", \n",
    "                        patience = 2, \n",
    "                        factor = 0.85, \n",
    "                        min_lr=1e-7,\n",
    "                        verbose=1)\n",
    "\n",
    "model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \n",
    "        filepath=\"./baseline_with_augmentation.h5\", #모델 파일 경로\n",
    "        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n",
    "        save_best_only=True)\n",
    "\n",
    "callbacks = [earlystop, learning_rate_reduction, model_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "latter-exhaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 270, 480, 32)      864       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 270, 480, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 270, 480, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 270, 480, 32)      9216      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 270, 480, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 270, 480, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 135, 240, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 135, 240, 64)      18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 135, 240, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 135, 240, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 135, 240, 64)      36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 135, 240, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 135, 240, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 67, 120, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 67, 120, 96)       55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 67, 120, 96)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 67, 120, 96)       384       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 67, 120, 96)       82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 67, 120, 96)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 67, 120, 96)       384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 33, 60, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 33, 60, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 33, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 33, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 33, 60, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 33, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 33, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 30, 256)       294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 30, 256)       589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 16, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 15, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 15, 512)        1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 15, 512)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 15, 512)        2048      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 15, 512)        2359296   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 8, 15, 512)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 15, 512)        2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 61440)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               31457792  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                24624     \n",
      "=================================================================\n",
      "Total params: 36,376,464\n",
      "Trainable params: 36,372,112\n",
      "Non-trainable params: 4,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    # Model Structure\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(270,480,3)))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(48))\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "controlled-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "valid.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "owned-protection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,32,270,480] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/leaky_re_lu_1/LeakyRelu (defined at <ipython-input-18-320f4cfb58d8>:9) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_2721]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-320f4cfb58d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,32,270,480] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/leaky_re_lu_1/LeakyRelu (defined at <ipython-input-18-320f4cfb58d8>:9) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_2721]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=10,\n",
    "                    validation_data=valid_dataset,\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "\n",
    "for test_path in tqdm(test_paths):\n",
    "    img=tf.io.read_file(test_path)\n",
    "    img=tf.image.decode_jpeg(img, channels=3)\n",
    "    img=tf.image.resize(img, [270,480])\n",
    "    img=img/255\n",
    "    X_test.append(img)\n",
    "\n",
    "X_test=tf.stack(X_test, axis=0)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission.iloc[:,1:] = pred * 4     # image size를 1920x1080 -> 480x270으로 바꿔서 예측했으므로 * 4\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('aug_rotation_noise_upsidedown.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
